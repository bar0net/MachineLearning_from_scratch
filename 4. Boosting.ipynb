{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "Boosting is a supervised learning algorithm that uses an ensamble of weak classifiers to predict a label. This is accomplished by successively training classifiers that focus on perfoming well on the subset of the data that the previous ones don't classify correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "\n",
    "import support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we are building a descrete classifier (labels are restricted to {-1,1}) that uses simple binary trees as weak classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteAdaBoost:\n",
    "    class __Weak:\n",
    "        # In Init we are going to create a simple decision tree\n",
    "        # that minimizes a weighted error\n",
    "        \n",
    "        def __init__(self, X, y, w):\n",
    "            self.prediction = dict()\n",
    "            cols = X.shape[1]\n",
    "            \n",
    "            error = np.array([float('Inf')] * cols)\n",
    "            isPositive = [False] * cols\n",
    "            \n",
    "            for i in range(cols):\n",
    "                pos = np.ones_like(y)\n",
    "                neg = -1 * np.ones_like(y)\n",
    "                \n",
    "                idx = (X[:,i] == 0)\n",
    "                pos[idx] = -1\n",
    "                neg[idx] =  1\n",
    "                \n",
    "                pos_error = np.sum((pos != y).astype('float') * w)\n",
    "                neg_error = np.sum((neg != y).astype('float') * w)\n",
    "                if pos_error < neg_error:\n",
    "                    error[i] = pos_error\n",
    "                    isPositive[i] = True\n",
    "                else:\n",
    "                    error[i] = neg_error\n",
    "            \n",
    "            self.index = np.argmin(error)\n",
    "            \n",
    "            if isPositive[self.index]:\n",
    "                self.prediction[0] = -1\n",
    "                self.prediction[1] = 1\n",
    "            else:\n",
    "                self.prediction[0] = 1\n",
    "                self.prediction[1] = -1            \n",
    "        \n",
    "        # Prediction value for the Weak classifier\n",
    "        def Predict(self, X):\n",
    "            output = [None] * np.shape(X)[0]\n",
    "\n",
    "            for i in range(np.shape(X)[0]):\n",
    "                value = X[i, self.index]\n",
    "                output[i] = self.prediction[value]\n",
    "            return np.array(output)\n",
    "    \n",
    "    def Fit(self, X, y, estimators = 3):\n",
    "        n = y.shape[0]\n",
    "        w = np.array([1/n] * n)\n",
    "        self.F = []\n",
    "        \n",
    "        self.__partitions = dict()\n",
    "        for i in range(X.shape[1]):\n",
    "            self.__partitions[i] = set(X[:,i])\n",
    "        Xb = self.__to_binary(X)\n",
    "        \n",
    "        for t in range(estimators):\n",
    "            weak = self.__Weak(Xb,y,w)\n",
    "            pred_weak = weak.Predict(Xb)\n",
    "            error = np.sum((pred_weak != y).astype('float') * w)\n",
    "            \n",
    "            if error == 0:\n",
    "                alpha = 5\n",
    "            else:\n",
    "                alpha = 0.5 * np.log( (1-error) / error )\n",
    "                \n",
    "            self.F.append( (alpha, weak) )\n",
    "            \n",
    "            w = w * np.exp(-y * alpha * pred_weak)\n",
    "            w = w / w.sum()\n",
    "    \n",
    "    def Predict(self, X):\n",
    "        counter = np.array( [0] * np.shape(X)[0])\n",
    "        Xb = self.__to_binary(X)\n",
    "        \n",
    "        for alpha, weak in self.F:\n",
    "            counter = counter + alpha * weak.Predict(Xb)\n",
    "        \n",
    "        predictions = np.ones_like(counter)\n",
    "        predictions[counter<0] = -1\n",
    "        \n",
    "        return predictions, counter / counter.max()\n",
    "    \n",
    "        \n",
    "    def __is_numeric(self, values):\n",
    "        return values.dtype.kind in set('buifc')\n",
    "    \n",
    "    def __to_binary(self, X):\n",
    "        output = np.empty((X.shape[0], 0))\n",
    "        \n",
    "        # Categorize each column\n",
    "        for i in range(X.shape[1]):            \n",
    "            # add a column \"feature < value\", for each value present in\n",
    "            # that feature\n",
    "            for value in self.__partitions[i]:\n",
    "                if self.__is_numeric(value):\n",
    "                    col = (X[:,i] < value).reshape((-1,1)).astype('int')\n",
    "                    output = np.hstack( (output, col))\n",
    "                else:\n",
    "                    col = (X[:,i] == value).reshape((-1,1)).astype('int')\n",
    "                    output = np.hstack( (output, col))\n",
    "                      \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 predictors - Training score: 0.8571428571428571\n",
      "2 predictors - Training score: 0.8571428571428571\n",
      "3 predictors - Training score: 1.0\n",
      "4 predictors - Training score: 1.0\n",
      "5 predictors - Training score: 1.0\n",
      "6 predictors - Training score: 1.0\n",
      "7 predictors - Training score: 1.0\n",
      "8 predictors - Training score: 1.0\n",
      "9 predictors - Training score: 1.0\n"
     ]
    }
   ],
   "source": [
    "X = [\n",
    "    [1,1,2,2,1],\n",
    "    [2,1,1,2,2],\n",
    "    [2,2,1,1,2],\n",
    "    [1,2,1,2,1],\n",
    "    [2,2,2,1,1],\n",
    "    [1,2,1,1,2],\n",
    "    [2,1,1,2,1]\n",
    "]\n",
    "\n",
    "y = [-1,1,1,-1,-1,1,1]\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "dab = DiscreteAdaBoost()\n",
    "\n",
    "for i in range(1,10):\n",
    "    dab.Fit(X,y, estimators=i)\n",
    "    pred, probs = dab.Predict(X)\n",
    "    print(i, \"predictors - Training score:\", (pred == y).sum() / y.shape[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 150 entries with 4 different features and 3 different labels.\n",
      "\n",
      "Dataset Head:\n",
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0                5.1               3.5                1.4               0.2\n",
      "1                4.9               3.0                1.4               0.2\n",
      "2                4.7               3.2                1.3               0.2\n",
      "3                4.6               3.1                1.5               0.2\n",
      "4                5.0               3.6                1.4               0.2\n"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "print(\"The dataset has\", X.shape[0], \"entries with\", X.shape[1], \"different features and\", len(set(y)), \"different labels.\\n\")\n",
    "\n",
    "print(\"Dataset Head:\")\n",
    "print(pd.DataFrame(X, columns=iris.feature_names).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aiolo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:64: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "C:\\Users\\aiolo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:68: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\aiolo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:80: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\aiolo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:78: RuntimeWarning: invalid value encountered in less\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 predictors - Training score: 1.0\n",
      "6 predictors - Training score: 1.0\n",
      "11 predictors - Training score: 1.0\n",
      "16 predictors - Training score: 1.0\n",
      "21 predictors - Training score: 1.0\n",
      "26 predictors - Training score: 1.0\n",
      "31 predictors - Training score: 1.0\n",
      "36 predictors - Training score: 1.0\n",
      "41 predictors - Training score: 1.0\n",
      "46 predictors - Training score: 1.0\n",
      "51 predictors - Training score: 1.0\n"
     ]
    }
   ],
   "source": [
    "dab = DiscreteAdaBoost()\n",
    "\n",
    "yd = np.ones_like(y)\n",
    "yd[y!=2] == -1\n",
    "\n",
    "for i in range(1,52,5):\n",
    "    dab.Fit(X,yd,estimators=i)\n",
    "    pred, probs = dab.Predict(X)\n",
    "    print(i, \"predictors - Training score:\", (pred == yd).sum() / yd.shape[0])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "[AdaBoost, Wikipedia](https://en.wikipedia.org/wiki/AdaBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
